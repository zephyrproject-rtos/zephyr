#!/usr/bin/env python3
# SPDX-FileCopyrightText: Copyright The Zephyr Project Contributors
# SPDX-License-Identifier: Apache-2.0

"""
Doxygen Coverage Diff Tool

Compares Doxygen documentation coverage between two coverxygen JSON reports
(base and PR branches) and identifies newly introduced undocumented API symbols.

Outputs GitHub Actions annotations for undocumented new symbols and fails if any
are found.

Usage:
    python3 doxygen_coverage_diff.py --base base.json --pr pr.json

The JSON files should be generated by coverxygen with --format json-v3.
"""

import argparse
import json
import os
import sys
from pathlib import Path


def load_coverage_json(path: str) -> dict:
    """Load a coverxygen json-v3 format file."""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def get_symbol_key(file_path: str, symbol: dict) -> tuple:
    """Create a unique key for a symbol based on file, name, and kind."""
    return (
        file_path,
        symbol.get("symbol", ""),  # coverxygen uses "symbol" not "name"
        symbol.get("kind", ""),
    )


def extract_symbols(coverage_data: dict) -> dict:
    """
    Extract all symbols from coverxygen json-v3 format.

    Returns a dict mapping symbol keys to a list of symbol info dicts.
    We use a list to handle overloaded symbols or duplicates within the same file.
    """
    symbols = {}

    files = coverage_data.get("files", {})

    for file_path, symbol_list in files.items():
        # In json-v3, file_data is a list of symbols directly
        if not isinstance(symbol_list, list):
            continue
        for symbol in symbol_list:
            key = get_symbol_key(file_path, symbol)
            if key not in symbols:
                symbols[key] = []
            symbols[key].append({
                "file": file_path,
                "line": symbol.get("line", 0),
                "name": symbol.get("symbol", ""),  # coverxygen uses "symbol" not "name"
                "kind": symbol.get("kind", ""),
                "documented": symbol.get("documented", False),
            })

    return symbols


def find_undocumented_new_symbols(base_symbols: dict, pr_symbols: dict) -> list:
    """
    Find symbols that are newly introduced and undocumented.

    This calculates the "net new" undocumented count PER FILE.
    If a file has N undocumented symbols in PR and M in Base:
    - If N <= M: We assume no new debt (ignores renames/refactors).
    - If N > M: We report the (N - M) new symbols.

    This matches the user intent to "not scream about an entire file"
    just because of macro expansion changes or renames.
    """
    from collections import Counter

    def get_undoc_by_file(all_symbols):
        by_file = {}
        for key, instances in all_symbols.items():
            # Filter for undocumented
            undoc = [s for s in instances if not s["documented"]]
            if not undoc:
                continue

            fname = key[0]  # key is (file, name, kind)
            if fname not in by_file:
                by_file[fname] = []
            by_file[fname].extend(undoc)
        return by_file

    base_undoc_by_file = get_undoc_by_file(base_symbols)
    pr_undoc_by_file = get_undoc_by_file(pr_symbols)

    undocumented_new = []

    # Check each file in PR that has undocumented symbols
    for filename, pr_list in pr_undoc_by_file.items():
        base_list = base_undoc_by_file.get(filename, [])

        net_increase = len(pr_list) - len(base_list)

        if net_increase > 0:
            # We need to find 'net_increase' number of candidates to blame.
            # Ideally, these are symbols present in PR but not Base.

            # Use Counter to handle potential duplicates (overloads) correctly
            # Key used for counting is the tuple (file, name, kind)
            base_counts = Counter((s["file"], s["name"], s["kind"]) for s in base_list)

            candidates = []
            for s in pr_list:
                k = (s["file"], s["name"], s["kind"])
                if base_counts[k] > 0:
                    base_counts[k] -= 1
                else:
                    candidates.append(s)

            # Report the candidates. If we have more candidates than the net increase
            # (e.g. we deleted 1 old undoc and added 2 new undoc -> net +1, candidates=2),
            # we limit to net_increase to follow the "net debt" philosophy strictly.
            # Sort candidates to be deterministic.
            candidates.sort(key=lambda s: s["line"])
            undocumented_new.extend(candidates[:net_increase])

    # Sort by file, then line for consistent output
    undocumented_new.sort(key=lambda s: (s["file"], s["line"]))

    return undocumented_new


def output_github_annotations(symbols: list, max_annotations: int = 50) -> None:
    """
    Output GitHub Actions workflow annotations for undocumented symbols.

    GitHub limits annotations to 50 per workflow, so we cap at max_annotations.
    """
    for i, symbol in enumerate(symbols[:max_annotations]):
        file_path = symbol["file"]
        line = symbol["line"]
        kind = symbol["kind"]
        name = symbol["name"]

        # GitHub Actions annotation format
        print(
            f"::error file={file_path},line={line},title=Missing Doxygen documentation::"
            f"{kind} '{name}' is not documented"
        )

    if len(symbols) > max_annotations:
        remaining = len(symbols) - max_annotations
        print(
            f"::warning::... and {remaining} more undocumented symbols "
            f"(showing first {max_annotations})"
        )


def write_summary(symbols: list, summary_file: str) -> None:
    """Write a markdown summary table to the given file (e.g., $GITHUB_STEP_SUMMARY)."""
    with open(summary_file, "a", encoding="utf-8") as f:
        f.write("\n## üìö Doxygen Coverage Check Results\n\n")

        if not symbols:
            f.write("‚úÖ **All new API symbols are properly documented!**\n")
            return

        f.write(
            f"> [!CAUTION]\n"
            f"> **{len(symbols)} new API symbol(s) are missing documentation.** "
            f"Please add Doxygen comments before merging.\n\n"
        )

        f.write("| File | Line | Kind | Symbol |\n")
        f.write("|------|------|------|--------|\n")

        # Limit table rows for readability
        max_rows = 50
        for symbol in symbols[:max_rows]:
            file_path = symbol["file"]
            line = symbol["line"]
            kind = symbol["kind"]
            name = symbol["name"]
            f.write(f"| `{file_path}` | {line} | {kind} | `{name}` |\n")

        if len(symbols) > max_rows:
            f.write(f"\n*... and {len(symbols) - max_rows} more*\n")

        f.write("\n### How to fix\n\n")
        f.write(
            "Add Doxygen documentation comments to the listed symbols. "
            "See the [Doxygen style guide](https://docs.zephyrproject.org/latest/contribute/style/doxygen.html) "
            "for guidance.\n"
        )


def main():
    parser = argparse.ArgumentParser(
        description="Compare Doxygen coverage between base and PR branches"
    )
    parser.add_argument(
        "--base",
        required=True,
        help="Path to base branch coverage JSON (coverxygen json-v3 format)",
    )
    parser.add_argument(
        "--pr",
        required=True,
        help="Path to PR branch coverage JSON (coverxygen json-v3 format)",
    )
    parser.add_argument(
        "--summary-file",
        help="Path to write markdown summary (e.g., $GITHUB_STEP_SUMMARY)",
    )

    args = parser.parse_args()

    # Check files exist
    if not os.path.isfile(args.base):
        print(f"::error::Base coverage file not found: {args.base}")
        sys.exit(1)

    if not os.path.isfile(args.pr):
        print(f"::error::PR coverage file not found: {args.pr}")
        sys.exit(1)

    # Load coverage data
    try:
        base_data = load_coverage_json(args.base)
        pr_data = load_coverage_json(args.pr)
    except json.JSONDecodeError as e:
        print(f"::error::Failed to parse coverage JSON: {e}")
        sys.exit(1)

    # Extract symbols
    base_symbols = extract_symbols(base_data)
    pr_symbols = extract_symbols(pr_data)

    print(f"Base branch: {len(base_symbols)} symbols")
    print(f"PR branch: {len(pr_symbols)} symbols")

    # Find undocumented new symbols
    undocumented_new = find_undocumented_new_symbols(base_symbols, pr_symbols)

    print(f"New undocumented symbols: {len(undocumented_new)}")

    # Output annotations
    if undocumented_new:
        output_github_annotations(undocumented_new)

    # Write summary if requested
    if args.summary_file:
        write_summary(undocumented_new, args.summary_file)

    # Exit with error if undocumented new symbols found
    if undocumented_new:
        print(
            f"\n‚ùå Found {len(undocumented_new)} new API symbol(s) without documentation."
        )
        print("Please add Doxygen comments to these symbols before merging.")
        sys.exit(1)
    else:
        print("\n‚úÖ All new API symbols are properly documented!")
        sys.exit(0)


if __name__ == "__main__":
    main()
